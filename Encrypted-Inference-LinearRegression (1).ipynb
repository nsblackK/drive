{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4945612e",
   "metadata": {},
   "source": [
    "<h1> Encrypted Inference-Linear Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdce7bf",
   "metadata": {},
   "source": [
    "Author:\n",
    "<ul>\n",
    "    <li>Hrishikesh Kamath - <a href=\"https://twitter.com/kamathhrishi\">Twitter</a> - <a href=\"https://github.com/kamathhrishi\">Github</a>\n",
    "</ul>\n",
    "Encrypted Inference is the process of performing inference with machine learning models such that model owner cannot observe the true input data nor can the data owner see the true model weights. The weights and data are encrypted by splitting them into shares and performiming computations according to a protocol. The general class of methods know as <b>Secure Multi Party Computation (SMPC)</b>. \n",
    "\n",
    "Below figure depicts MPC for ML models for 2 parties. \n",
    "\n",
    "<img height=\"600px\" width=\"600px\" src=\"Images/smpc_illustration.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943011cc",
   "metadata": {},
   "source": [
    "In this example we use Virtual Machine to demonstrate performing inference using SMPC. That is the workers are present on the same PC. If you want to understand how to perform remotely check out duet tutorials. \n",
    "In this example, we train a Linear regression model in plaintext on Boston Housing Dataset. Then we use the model for performing encrypted inference on test data. This tutorial uses protocol Falcon for 3 parties and SPDZ for 3 and 5 parties. \n",
    "\n",
    "In SyMPC the computation between parties occurs using a orchestrator which describes how computations should take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df3753ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running loss is nan\n",
    "#batch size\n",
    "#Define Linear regression model (figure out)\n",
    "#criterion = torch.nn.MSELoss(reduction='mean') \n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cfe9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "#External libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b25677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67713542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1244151990>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set a manual seed to maintain consistency\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6b5a2a",
   "metadata": {},
   "source": [
    "<h2>Data Loading and Processing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39d5bb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://deb.debian.org/debian buster InRelease\n",
      "Get:2 http://security.debian.org/debian-security buster/updates InRelease [65.4 kB]\n",
      "Get:3 http://deb.debian.org/debian buster-updates InRelease [51.9 kB]\n",
      "Get:4 http://security.debian.org/debian-security buster/updates/main amd64 Packages [308 kB]\n",
      "Fetched 426 kB in 1s (346 kB/s)   \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  wget\n",
      "0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\n",
      "Need to get 902 kB of archives.\n",
      "After this operation, 3335 kB of additional disk space will be used.\n",
      "Get:1 http://deb.debian.org/debian buster/main amd64 wget amd64 1.20.1-1.1 [902 kB]\n",
      "Fetched 902 kB in 0s (2588 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package wget.\n",
      "(Reading database ... 20003 files and directories currently installed.)\n",
      "Preparing to unpack .../wget_1.20.1-1.1_amd64.deb ...\n",
      "Unpacking wget (1.20.1-1.1) ...\n",
      "Setting up wget (1.20.1-1.1) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get update\n",
    "!apt-get install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c09bbb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-11 10:42:14--  https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 49082 (48K) [application/x-httpd-php]\n",
      "Saving to: ‚Äòhousing.data‚Äô\n",
      "\n",
      "housing.data        100%[===================>]  47.93K   107KB/s    in 0.4s    \n",
      "\n",
      "2022-01-11 10:42:16 (107 KB/s) - ‚Äòhousing.data‚Äô saved [49082/49082]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Download Boston housing dataset\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a38de5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataset and add headers\n",
    "dataset=pd.read_csv(\"housing.data\",delim_whitespace=True,\n",
    "                    names=[\"crim\",\"zn\",\"indus\",\n",
    "                           \"chas\",\"nox\",\"rm\",\n",
    "                           \"age\",\"dis\",\"rad\",\n",
    "                           \"tax\",\"ptratio\",\"black\",\n",
    "                           \"lstat\",\"medv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "860112db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad    tax  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   ptratio   black  lstat  medv  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualize and look at columns and rows of dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03e1045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into features and target variables\n",
    "features = dataset.drop(\"medv\",axis=1)\n",
    "targets = dataset[\"medv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f0ed65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize features\n",
    "features = features.apply(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7c38c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.419367</td>\n",
       "      <td>0.284548</td>\n",
       "      <td>-1.286636</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.144075</td>\n",
       "      <td>0.413263</td>\n",
       "      <td>-0.119895</td>\n",
       "      <td>0.140075</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.665949</td>\n",
       "      <td>-1.457558</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-1.074499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.416927</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-0.592794</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.739530</td>\n",
       "      <td>0.194082</td>\n",
       "      <td>0.366803</td>\n",
       "      <td>0.556609</td>\n",
       "      <td>-0.867024</td>\n",
       "      <td>-0.986353</td>\n",
       "      <td>-0.302794</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.491953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.416929</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-0.592794</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.739530</td>\n",
       "      <td>1.281446</td>\n",
       "      <td>-0.265549</td>\n",
       "      <td>0.556609</td>\n",
       "      <td>-0.867024</td>\n",
       "      <td>-0.986353</td>\n",
       "      <td>-0.302794</td>\n",
       "      <td>0.396035</td>\n",
       "      <td>-1.207532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.416338</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-1.305586</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.834458</td>\n",
       "      <td>1.015298</td>\n",
       "      <td>-0.809088</td>\n",
       "      <td>1.076671</td>\n",
       "      <td>-0.752178</td>\n",
       "      <td>-1.105022</td>\n",
       "      <td>0.112920</td>\n",
       "      <td>0.415751</td>\n",
       "      <td>-1.360171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.412074</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-1.305586</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.834458</td>\n",
       "      <td>1.227362</td>\n",
       "      <td>-0.510674</td>\n",
       "      <td>1.076671</td>\n",
       "      <td>-0.752178</td>\n",
       "      <td>-1.105022</td>\n",
       "      <td>0.112920</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-1.025487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>-0.412820</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>0.157968</td>\n",
       "      <td>0.438881</td>\n",
       "      <td>0.018654</td>\n",
       "      <td>-0.625178</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.386834</td>\n",
       "      <td>-0.417734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>-0.414839</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>0.157968</td>\n",
       "      <td>-0.234316</td>\n",
       "      <td>0.288648</td>\n",
       "      <td>-0.715931</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.500355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>-0.413038</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>0.157968</td>\n",
       "      <td>0.983986</td>\n",
       "      <td>0.796661</td>\n",
       "      <td>-0.772919</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.982076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>-0.407361</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>0.157968</td>\n",
       "      <td>0.724955</td>\n",
       "      <td>0.736268</td>\n",
       "      <td>-0.667776</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.402826</td>\n",
       "      <td>-0.864446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>-0.414590</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>0.115624</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>0.157968</td>\n",
       "      <td>-0.362408</td>\n",
       "      <td>0.434302</td>\n",
       "      <td>-0.612640</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.668397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows √ó 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         crim        zn     indus      chas       nox        rm       age  \\\n",
       "0   -0.419367  0.284548 -1.286636 -0.272329 -0.144075  0.413263 -0.119895   \n",
       "1   -0.416927 -0.487240 -0.592794 -0.272329 -0.739530  0.194082  0.366803   \n",
       "2   -0.416929 -0.487240 -0.592794 -0.272329 -0.739530  1.281446 -0.265549   \n",
       "3   -0.416338 -0.487240 -1.305586 -0.272329 -0.834458  1.015298 -0.809088   \n",
       "4   -0.412074 -0.487240 -1.305586 -0.272329 -0.834458  1.227362 -0.510674   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "501 -0.412820 -0.487240  0.115624 -0.272329  0.157968  0.438881  0.018654   \n",
       "502 -0.414839 -0.487240  0.115624 -0.272329  0.157968 -0.234316  0.288648   \n",
       "503 -0.413038 -0.487240  0.115624 -0.272329  0.157968  0.983986  0.796661   \n",
       "504 -0.407361 -0.487240  0.115624 -0.272329  0.157968  0.724955  0.736268   \n",
       "505 -0.414590 -0.487240  0.115624 -0.272329  0.157968 -0.362408  0.434302   \n",
       "\n",
       "          dis       rad       tax   ptratio     black     lstat  \n",
       "0    0.140075 -0.981871 -0.665949 -1.457558  0.440616 -1.074499  \n",
       "1    0.556609 -0.867024 -0.986353 -0.302794  0.440616 -0.491953  \n",
       "2    0.556609 -0.867024 -0.986353 -0.302794  0.396035 -1.207532  \n",
       "3    1.076671 -0.752178 -1.105022  0.112920  0.415751 -1.360171  \n",
       "4    1.076671 -0.752178 -1.105022  0.112920  0.440616 -1.025487  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "501 -0.625178 -0.981871 -0.802418  1.175303  0.386834 -0.417734  \n",
       "502 -0.715931 -0.981871 -0.802418  1.175303  0.440616 -0.500355  \n",
       "503 -0.772919 -0.981871 -0.802418  1.175303  0.440616 -0.982076  \n",
       "504 -0.667776 -0.981871 -0.802418  1.175303  0.402826 -0.864446  \n",
       "505 -0.612640 -0.981871 -0.802418  1.175303  0.440616 -0.668397  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e7111a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      24.0\n",
       "1      21.6\n",
       "2      34.7\n",
       "3      33.4\n",
       "4      36.2\n",
       "       ... \n",
       "501    22.4\n",
       "502    20.6\n",
       "503    23.9\n",
       "504    22.0\n",
       "505    11.9\n",
       "Name: medv, Length: 506, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "085f5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert features and targets into torch tensors\n",
    "features = torch.tensor(features.values.astype(np.float32)) \n",
    "targets = torch.tensor(targets.values.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d775369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for training\n",
    "batch_size = 16\n",
    "epochs = 300\n",
    "train_test_split = 0.8\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83aaf309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset into train and test\n",
    "train_indices=int(len(features)*train_test_split)\n",
    "\n",
    "train_x = features[:train_indices]\n",
    "train_y = targets[:train_indices]\n",
    "\n",
    "test_x = features[train_indices+1:]\n",
    "test_y = targets[train_indices+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d12eb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter number of models to b created: 5\n"
     ]
    }
   ],
   "source": [
    "nom = int(input(\"Enter number of models to b created: \"))\n",
    "m=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "034b0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain=[]\n",
    "xtrain.append(np.array_split(train_x, nom))\n",
    "\n",
    "ytrain=[]\n",
    "ytrain.append(np.array_split(train_y, nom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "394b765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide dataset into batches\n",
    "def get_batches(X, y):\n",
    "    batches = []\n",
    "    for index in range(0,len(train_x)+1,batch_size):\n",
    "        batches.append((X[index:index+batch_size],y[index:index+batch_size]))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e98f4a3",
   "metadata": {},
   "source": [
    "<h1>Plaintext Training</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fba7d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import syft\n",
    "import syft as sy\n",
    "sy.logger.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f9c72b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Linear regression model\n",
    "class LinearSyNet(sy.Module):\n",
    "    def __init__(self, torch_ref):\n",
    "        super(LinearSyNet, self).__init__(torch_ref=torch_ref)\n",
    "        self.fc1 = self.torch_ref.nn.Linear(13,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8636bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model, loss function and optimizer\n",
    "for i in range(nom):\n",
    "    model = LinearSyNet(torch)\n",
    "    m.append(model)\n",
    "criterion = torch.nn.MSELoss(reduction='mean') \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f44dab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest=[]\n",
    "xtest.append(np.array_split(test_x, nom))\n",
    "\n",
    "ytest=[]\n",
    "ytest.append(np.array_split(test_y, nom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5ba6086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0037, -0.4872,  1.0150, -0.2723,  0.2529, -0.6371, -0.3153, -0.8536,\n",
      "          1.6596,  1.5294,  0.8058, -3.6368,  0.4253],\n",
      "        [ 3.9584, -0.4872,  1.0150, -0.2723,  1.0727, -0.1176,  0.3597, -0.9176,\n",
      "          1.6596,  1.5294,  0.8058, -3.7007,  0.2614],\n",
      "        [ 0.4364, -0.4872,  1.0150, -0.2723,  1.0727, -0.1304,  0.3384, -0.8830,\n",
      "          1.6596,  1.5294,  0.8058, -2.8473,  1.2417],\n",
      "        [ 0.6656, -0.4872,  1.0150, -0.2723,  1.0727,  0.1357,  0.9601, -0.8676,\n",
      "          1.6596,  1.5294,  0.8058, -3.2417,  1.6002],\n",
      "        [ 0.5672, -0.4872,  1.0150, -0.2723,  0.2529,  0.0902,  0.6226, -0.8274,\n",
      "          1.6596,  1.5294,  0.8058, -2.9928,  0.6983],\n",
      "        [ 0.7497, -0.4872,  1.0150, -0.2723,  0.2529,  0.7805,  0.9139, -0.8106,\n",
      "          1.6596,  1.5294,  0.8058, -3.0160,  0.9854],\n",
      "        [ 0.3291, -0.4872,  1.0150, -0.2723,  0.2529,  0.1998,  0.2211, -0.7573,\n",
      "          1.6596,  1.5294,  0.8058, -2.8339, -0.0873],\n",
      "        [ 0.2287, -0.4872,  1.0150, -0.2723,  1.3661,  0.2154,  0.6865, -0.7025,\n",
      "          1.6596,  1.5294,  0.8058, -2.8094,  0.4995],\n",
      "        [ 1.1974, -0.4872,  1.0150, -0.2723,  1.3661, -0.1091,  0.9388, -0.7469,\n",
      "          1.6596,  1.5294,  0.8058, -2.8046,  0.3525],\n",
      "        [ 0.8774, -0.4872,  1.0150, -0.2723,  1.5991,  0.4901,  0.9246, -0.7932,\n",
      "          1.6596,  1.5294,  0.8058, -2.7036,  1.4867],\n",
      "        [ 1.2564, -0.4872,  1.0150, -0.2723,  1.5991,  0.2510,  0.8784, -0.8512,\n",
      "          1.6596,  1.5294,  0.8058, -3.6057,  0.7558],\n",
      "        [ 1.3444, -0.4872,  1.0150, -0.2723,  1.5991, -0.1888,  1.1164, -0.8932,\n",
      "          1.6596,  1.5294,  0.8058, -3.8047,  1.9321],\n",
      "        [ 1.1701, -0.4872,  1.0150, -0.2723,  1.5991, -0.4976,  0.6865, -0.9377,\n",
      "          1.6596,  1.5294,  0.8058, -3.1516,  2.9921],\n",
      "        [ 0.6716, -0.4872,  1.0150, -0.2723,  1.5991, -0.9360,  0.8997, -0.9393,\n",
      "          1.6596,  1.5294,  0.8058,  0.4406,  1.4321],\n",
      "        [ 2.1435, -0.4872,  1.0150, -0.2723,  1.5991, -0.6641,  0.8464, -0.9160,\n",
      "          1.6596,  1.5294,  0.8058,  0.3809,  1.3243],\n",
      "        [ 0.7104, -0.4872,  1.0150, -0.2723,  1.5991,  0.1727,  1.0169, -0.8215,\n",
      "          1.6596,  1.5294,  0.8058,  0.3208,  0.9616],\n",
      "        [ 0.2387, -0.4872,  1.0150, -0.2723,  1.5991, -0.0934,  1.1164, -0.8502,\n",
      "          1.6596,  1.5294,  0.8058,  0.4274,  0.5513],\n",
      "        [ 0.7386, -0.4872,  1.0150, -0.2723,  1.5991,  0.2852,  1.1164, -0.8627,\n",
      "          1.6596,  1.5294,  0.8058,  0.3292,  0.8678],\n",
      "        [ 1.0683, -0.4872,  1.0150, -0.2723,  1.5991, -0.6129,  0.9956, -0.9020,\n",
      "          1.6596,  1.5294,  0.8058, -1.2723,  1.5596],\n",
      "        [ 0.8206, -0.4872,  1.0150, -0.2723,  1.5991,  0.2482,  0.9317, -0.8582,\n",
      "          1.6596,  1.5294,  0.8058, -3.4352,  1.5862]])\n"
     ]
    }
   ],
   "source": [
    "print(xtest[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4192669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  0\n",
      "Epoch 0/300  Running Loss : nan and test loss: 257.4114990234375\n",
      "Epoch 50/300  Running Loss : nan and test loss: 257.4114990234375\n",
      "Epoch 100/300  Running Loss : nan and test loss: 257.4114990234375\n",
      "Epoch 150/300  Running Loss : nan and test loss: 257.4114990234375\n",
      "Epoch 200/300  Running Loss : nan and test loss: 257.4114990234375\n",
      "Epoch 250/300  Running Loss : nan and test loss: 257.4114990234375\n",
      "model:  1\n",
      "Epoch 0/300  Running Loss : nan and test loss: 169.77113342285156\n",
      "Epoch 50/300  Running Loss : nan and test loss: 169.77113342285156\n",
      "Epoch 100/300  Running Loss : nan and test loss: 169.77113342285156\n",
      "Epoch 150/300  Running Loss : nan and test loss: 169.77113342285156\n",
      "Epoch 200/300  Running Loss : nan and test loss: 169.77113342285156\n",
      "Epoch 250/300  Running Loss : nan and test loss: 169.77113342285156\n",
      "model:  2\n",
      "Epoch 0/300  Running Loss : nan and test loss: 282.78448486328125\n",
      "Epoch 50/300  Running Loss : nan and test loss: 282.78448486328125\n",
      "Epoch 100/300  Running Loss : nan and test loss: 282.78448486328125\n",
      "Epoch 150/300  Running Loss : nan and test loss: 282.78448486328125\n",
      "Epoch 200/300  Running Loss : nan and test loss: 282.78448486328125\n",
      "Epoch 250/300  Running Loss : nan and test loss: 282.78448486328125\n",
      "model:  3\n",
      "Epoch 0/300  Running Loss : nan and test loss: 385.4523010253906\n",
      "Epoch 50/300  Running Loss : nan and test loss: 385.4523010253906\n",
      "Epoch 100/300  Running Loss : nan and test loss: 385.4523010253906\n",
      "Epoch 150/300  Running Loss : nan and test loss: 385.4523010253906\n",
      "Epoch 200/300  Running Loss : nan and test loss: 385.4523010253906\n",
      "Epoch 250/300  Running Loss : nan and test loss: 385.4523010253906\n",
      "model:  4\n",
      "Epoch 0/300  Running Loss : nan and test loss: 48.473018646240234\n",
      "Epoch 50/300  Running Loss : nan and test loss: 48.18556594848633\n",
      "Epoch 100/300  Running Loss : nan and test loss: 47.93592071533203\n",
      "Epoch 150/300  Running Loss : nan and test loss: 47.721580505371094\n",
      "Epoch 200/300  Running Loss : nan and test loss: 47.540313720703125\n",
      "Epoch 250/300  Running Loss : nan and test loss: 47.38981246948242\n"
     ]
    }
   ],
   "source": [
    "#Training Loop\n",
    "for i in range(nom):\n",
    "    train_batches=get_batches(xtrain[0][i],ytrain[0][i])\n",
    "    print(\"model: \", i)\n",
    "    for epoch in range(epochs):\n",
    "      running_loss = 0.0\n",
    "      for index in range(0,len(train_batches)):\n",
    "        # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get output from the model, given the inputs\n",
    "        outputs = m[i](train_batches[index][0]).reshape([-1])\n",
    "\n",
    "        # get loss for the predicted output\n",
    "        loss = criterion(outputs,train_batches[index][1])\n",
    "        running_loss += loss\n",
    "        # get gradients w.r.t to parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "      test_accuracy = criterion(m[i](xtest[0][i]).reshape([-1]),ytest[0][i])#use xtest and ytest\n",
    "      if((epoch%50)==0):\n",
    "         print(f\"Epoch {epoch}/{epochs}  Running Loss : {running_loss.item()/batch_size} and test loss: {test_accuracy.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3a2e0c",
   "metadata": {},
   "source": [
    "<h1>Encrypted Inference</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4ba3c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SyMPC imports required for encrypted inference\n",
    "import sympc\n",
    "from sympc.session import Session\n",
    "from sympc.session import SessionManager\n",
    "from sympc.tensor import MPCTensor\n",
    "from sympc.protocol import Falcon,FSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "099f2b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clients(n_parties):\n",
    "  #Generate required number of syft clients and return them.\n",
    "\n",
    "  parties=[]\n",
    "  for index in range(n_parties): \n",
    "      parties.append(sy.VirtualMachine(name = \"worker\"+str(index)).get_root_client())\n",
    "\n",
    "  return parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6cbab6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_send(data,session):\n",
    "    \"\"\"Splits data into number of chunks equal to number of parties and distributes it to respective \n",
    "       parties.\n",
    "    \"\"\"\n",
    "    data_pointers = []\n",
    "    \n",
    "    split_size = int(len(data)/len(session.parties))+1\n",
    "    for index in range(0,len(session.parties)):\n",
    "        ptr=data[index*split_size:index*split_size+split_size].share(session=session)\n",
    "        data_pointers.append(ptr)\n",
    "        \n",
    "    return data_pointers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49737ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "632d366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(n_clients,protocol=None):\n",
    "    \n",
    "  # Get VM clients \n",
    "  parties=get_clients(n_clients)\n",
    "\n",
    "  # Setup the session for the computation\n",
    "  if(protocol):\n",
    "     session = Session(parties = parties,protocol = protocol)\n",
    "  else:\n",
    "     session = Session(parties = parties)\n",
    "        \n",
    "  SessionManager.setup_mpc(session)\n",
    "\n",
    "  for i in range(nom):\n",
    "        #Split data and send data to clients\n",
    "        pointers = split_send(xtest[0][i],session)\n",
    "\n",
    "        #Encrypt model \n",
    "        mpc_model = m[i].share(session)\n",
    "\n",
    "        #Encrypt test data\n",
    "        #test_data=MPCTensor(secret=test_x, session = session)\n",
    "\n",
    "        #Perform inference and measure time taken\n",
    "        start_time = time.time()\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for ptr in pointers:\n",
    "            encrypted_results = mpc_model(ptr)\n",
    "            plaintext_results = encrypted_results.reconstruct()\n",
    "            results.append(plaintext_results)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(f\"Time for inference: {end_time-start_time}s\")\n",
    "\n",
    "        predictions = torch.cat(results).reshape([-1])\n",
    "\n",
    "        #Calculate Loss\n",
    "        print(\"MSE Loss: \",criterion(predictions,ytest[0][i]).item())\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b35ec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference: 0.1719961166381836s\n",
      "MSE Loss:  257.4112243652344\n"
     ]
    }
   ],
   "source": [
    "predictions=inference(3,Falcon(\"semi-honest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec10fe5a",
   "metadata": {},
   "source": [
    "We can see that the prediction values and mean squared error values are almost the same as final model. Small differences are due to precision loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1bea240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0\n",
      "Encrypted Prediction Output -1.28875732421875\n",
      "Expected Prediction: 5.0\n",
      "\n",
      "\n",
      "Index 1\n",
      "Encrypted Prediction Output -1.43994140625\n",
      "Expected Prediction: 11.899999618530273\n",
      "\n",
      "\n",
      "Index 2\n",
      "Encrypted Prediction Output -0.8323516845703125\n",
      "Expected Prediction: 27.899999618530273\n",
      "\n",
      "\n",
      "Index 3\n",
      "Encrypted Prediction Output -1.2740936279296875\n",
      "Expected Prediction: 17.200000762939453\n",
      "\n",
      "\n",
      "Index 4\n",
      "Encrypted Prediction Output -0.817962646484375\n",
      "Expected Prediction: 27.5\n",
      "\n",
      "\n",
      "Index 5\n",
      "Encrypted Prediction Output -0.48388671875\n",
      "Expected Prediction: 15.0\n",
      "\n",
      "\n",
      "Index 6\n",
      "Encrypted Prediction Output -0.7993011474609375\n",
      "Expected Prediction: 17.200000762939453\n",
      "\n",
      "\n",
      "Index 7\n",
      "Encrypted Prediction Output -1.4966278076171875\n",
      "Expected Prediction: 17.899999618530273\n",
      "\n",
      "\n",
      "Index 8\n",
      "Encrypted Prediction Output -1.01806640625\n",
      "Expected Prediction: 16.299999237060547\n",
      "\n",
      "\n",
      "Index 9\n",
      "Encrypted Prediction Output -1.724609375\n",
      "Expected Prediction: 7.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index in range(0,10):\n",
    "    print(f\"Index {index}\")\n",
    "    print(f\"Encrypted Prediction Output {predictions[index].item()}\")\n",
    "    #print(f\"Plaintext Prediction Output {plaintext_predictions[index].item()}\")\n",
    "    print(f\"Expected Prediction: {test_y[index]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3de12f",
   "metadata": {},
   "source": [
    "<h1> Conclusion </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017c5bb7",
   "metadata": {},
   "source": [
    "Falcon can also provide a malicious security guarantee for an honest majority at the cost of higher inference time. Malicious security ensures that all the parties compute according to the protocol and do not deviate from protocol or tamper with shares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0671bd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference: 0.536308765411377s\n",
      "MSE Loss:  257.4112854003906\n"
     ]
    }
   ],
   "source": [
    "predictions=inference(3,Falcon(\"malicious\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e543e93",
   "metadata": {},
   "source": [
    "When we do not pass any protocol to session, SyMPC uses SPDZ and FSS protocol with semi-honest security type. \n",
    "\n",
    "SPDZ is used for multiplication and related operations (convolution,matmul,etc).\n",
    "Functional Secret Sharing (FSS) for other operations such as comparison, equality, maxpool, etc. \n",
    "\n",
    "FSS works only for 2 parties while SPDZ could extend to N parties. \n",
    "\n",
    "Linear regression uses only matmul which utilizes SPDZ protocol allowing us to run linear regression with several parties in this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "038bbe3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference: 0.3889961242675781s\n",
      "MSE Loss:  257.4106750488281\n"
     ]
    }
   ],
   "source": [
    "predictions=inference(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "398a2aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference: 0.7254760265350342s\n",
      "MSE Loss:  257.4106750488281\n"
     ]
    }
   ],
   "source": [
    "predictions=inference(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8a8083d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference: 1.1546521186828613s\n",
      "MSE Loss:  257.4105224609375\n"
     ]
    }
   ],
   "source": [
    "predictions=inference(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8325a69",
   "metadata": {},
   "source": [
    "<center><h3> Comparison </h3></center>\n",
    "\n",
    "| Protocol | Security Type| Parties | Inference Time (s) |\n",
    "| --- | --- | --- | --- |\n",
    "| Plaintext | |  | 0.000534|\n",
    "| Falcon | Semi-honest | 3 | 0.118 |\n",
    "| Falcon | Malicious | 3 | 0.6659 |\n",
    "| SPDZ| Semi-honest | 3 | 0.4993 |\n",
    "| SPDZ | Semi-honest | 5 | 1.3192|\n",
    "\n",
    "<b>Note:</b> The above table is only for comparison. The inference time varies for different PC specs and CPU load.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092cecc",
   "metadata": {},
   "source": [
    "Falcon provides faster inference for a 3 parties setting. While, SPDZ allows inference for N number of parties. Both allow inference with almost same accuracy as plaintext. \n",
    "\n",
    "Although Falcon is much more faster, it isn't scalable because it is applicable for only 3 parties. Further, it is less secure. Since it uses 2-out-of-3 sharing where each party recieves two shares allowing 2 parties to reconstruct a secret without other party knowing. Falcon also assumes that majority of parties are honest (2 in this case). \n",
    "\n",
    "While, SPDZ and FSS distributes a single share to every party requiring shares from all parties for reconstruction ensuring no parties could collude. Currently SyMPC provides support for SPDZ and FSS with semi-honest security guarantee. This allows parties to tamper with shares leading to incorrect results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef4aaf",
   "metadata": {},
   "source": [
    "<h3>What's next?</h3>\n",
    "\n",
    "SyMPC is still under development! We will add here more features as soon they are stable enough, stay tuned! üï∫\n",
    "\n",
    "If you enjoyed this tutorial, show your support by Starring SyMPC! üôè"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
